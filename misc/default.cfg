# Default configuration for QUAC stuff. (In case it's not obvious, don't start
# editing this file to add your configuration.)
#
# ALL paths in this file are relative to the directory containing the
# configuration file specified on the command line (NOT the config file
# currently being read).
#
# Importantly, that means no paths can be set in default.cfg, because there
# might be no --config option.
#
# Copyright (c) Los Alamos National Security, LLC, and others.


[path]
### Various paths.

# Load this other config file (only for the config file specified on the
# command line).
next_config =

# Location of the log file. (Note that more detailed logs are also printed to
# stdout, if it's a TTY.)
log =


[limt]
### General limits.

# SQLite page cache size (KiB)
# http://www.sqlite.org/pragma.html#pragma_cache_size
sqlite_page_cache_kb = 1048576

# SQLite head size soft limit (bytes)
# http://www.sqlite.org/malloc.html
# http://www.sqlite.org/c3ref/soft_heap_limit64.html
sqlite_heap_bytes = 1073741824


[coll]
### Configuration for the collector.

# Directory in which to store collected tweet files.
tweets_dir =

# Parameters for OAuth login
consumer_key =
consumer_secret =
access_token =
access_secret =

# Location of the file containing keywords. If this is blank, a
# statuses/sample stream is started (i.e., random sample of all tweets);
# otherwise, a statuses/filter stream with the keywords in the file.
keywords_file =

# Maximum number of keywords to track. This just mirrors the Twitter limit
# (see https://dev.twitter.com/docs/streaming-api/methods) in order to provide
# more useful errors.
keywords_limit = 400

# How many tweets to store per file? (Currently, this is tuned to yield
# approximately 5 files per day with a 1% sample.)
tweets_per_file = 500000

# Log a heartbeat at the DEBUG level at this interval. Must be a power of two
# (in order to support our simple algorithm for heartbeating more frequently
# at startup.)
seconds_per_heartbeat = 256  # must be a power of two


## Some network parameters.

# If true, do not use a proxy even if the environment suggests one.
no_proxy = false

# Reconnect backoff parameters.
#
# Initial delay (in seconds) after a stable connection fails.
reconnect_delay_base = 10.0
# Each time we reconnect again before reaching a stable connection, wait this
# many times longer than the last reconnect delay...
reconnect_delay_mult = 2.0
# ... until we are waiting this long (in seconds), which is the maximum. If we
# actually reach this, emit a warning each time, but don't stop retrying.
reconnect_delay_max = 21600  # 6 hours

# A connection which stays up this long (in seconds) is assumed to be stable,
# and we reset the backoff.
connect_ok_duration = 180

# This limits the number of connections before we assume something significant
# is wrong and abort. Specifically, if more than _limit connections happen
# within _interval seconds, abort.
#
# Note that this is for detecting connection problems that lead to rapid
# bouncing. For a slow failure in which a stable connection can't be achieved
# and we keep waiting longer and longer, you want the warnings from
# reconnect_delay_max instead.
connect_limit = 48              # average interval of 5 minutes for ...
connect_limit_interval = 14400  # 4 hours


[pars]
### Configuration for the raw tweet parser

# The loader is tolerant of parsing failures, but if more than this many occur
# per file, abort.
parse_failure_max = 10


[tops]
### Configuration for the Topsy API

apikey =

# See Topsy API docs...
count_method = citation


[wkpd]
### Wikipedia-related

# Note that rsync with no destination argument works like "ls". This is handy
# to (a) explore what a mirror has and (b) make sure rsync works.

# Local path to Wikipedia access log collection.
access_log_dir =

# URL of access log mirror
access_log_url = rsync://wikimedia.wansec.com/wikimedia/other/pagecounts-raw/

# Local path to XML dumps
dump_dir =

# URL of XML dump mirror
dump_url = rsync://ftpmirror.your.org/wikimedia-dumps/

# Bandwidth limit (KBytes per second). The default is deliberately low so that
# one has to take specific action to be more aggressive. Note that *each*
# download process will consume this much bandwidth (unless someone else is
# capping it).
bandwidth_limit = 256

# Which projects to fetch dumps for? The default is two small Wikipedias
# (Maori and Manx Gaelic) to get you started. It is recommended you get things
# working with these, and then scale up to the much larger wikis that you
# probably want.
projects = miwiki
           gvwiki

# Which XML dumps to fetch? See <http://dumps.wikimedia.org/enwiki/20130503/>
# for examples of what is available. This a glob pattern matching the
# filenames you want; the project and date prefix (or a subpattern matching
# this) will be prepeded. For example, if the project is "enwiki",
# "pages-meta-history*.7z" will be translated to
# "enwiki-latest-pages-meta-history*.7z" and will download the "all pages with
# complete edit history" files in 7zip format.
#
# WARNING: Writing these patterns takes some finesse. In particular:
#
# * Watch out for over-broad patterns that download more than you want. For
#   example, "pages-meta-history*" for enwiki will download the complete edit
#   history files in both 7zip and the much larger bzip2.
#
# * Each pattern is applied identically to each project. This can have
#   surprising effects because some files are split in large wikis but unsplit
#   in small wikis.
#
#   For example, "stub-meta-history*.xml.gz" will get you the split revision
#   metadata files in enwiki, but nothing in miwki, because that file is not
#   split for that Wikipedia. However, enwiki also has an unsplit version, so
#   you can use "stub-meta-history.xml.gz" and it will work for both.
#
#   On the other hand, for complete edit history, enwiki has only the split
#   version and miwiki has only the unsplit version, so for this file you can
#   use a pattern that matches both.
dumps = stub-meta-history.xml.gz

# HDF5 files per month of pageview data
hashmod = 256

# Articles with fewer than this many hits per month are discarded from time
# series datasets, though they are not guaranteed to be absent. See
# wp-tsupdate for details of the algorithm.
keep_threshold = 60
